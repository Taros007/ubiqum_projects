set.seed(998)
#load library and set seed
library(caret)
set.seed(998)
#create a 20% sample of the data
WholeYear <- WholeYear[sample(1:nrow(WholeYear), 7000,replace=FALSE),]
# define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(WholeYear$SolarRad, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#dataframe for manual tuning of mtry
rfGrid <- expand.grid(mtry=c(1,2,3))
#train Random Forest Regression model
#note the system time wrapper. system.time()
#this is used to measure process execution time
system.time(rfFitm1 <- train(SolarRad~., data = training, method = "rf", trControl=fitControl, tuneGrid=rfGrid))
WholeYear = read.csv('./input/WholeYear.csv')
# Required
library(doParallel)
# Create Cluster with desired number of cores. Don't use them all! Your computer is running other processes.
cl <- makeCluster(3)
# Register Cluster
registerDoParallel(cl)
#load library and set seed
library(caret)
set.seed(998)
#create a 20% sample of the data
WholeYear <- WholeYear[sample(1:nrow(WholeYear), 7000,replace=FALSE),]
# define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(WholeYear$SolarRad, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(SolarRad~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
?randomFores
?randomForest
rfFit1
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(SolarRad~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
rfFit1
library("caret")
?train
WholeYear = read.csv('./input/WholeYear.csv')
# Required
library(doParallel)
# Create Cluster with desired number of cores. Don't use them all! Your computer is running other processes.
cl <- makeCluster(3)
# Register Cluster
registerDoParallel(cl)
#load library and set seed
library(caret)
set.seed(998)
#create a 20% sample fo the data
WholeYear <- WholeYear[sample(1:nrow(WholeYear), 7000,replace=FALSE),]
#define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(WholeYear$SolarRad, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
rfitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, search = 'random')
#train Random Forest Regression model
rfFitr2 <- train(SolarRad~., data = training, method = "rf", trControl=rfitControl)
rfFitr2
library(caret)
set.seed(998)
#create a 20% sample fo the data
WholeYear <- WholeYear[sample(1:nrow(WholeYear), 7000,replace=FALSE),]
# define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(WholeYear$SolarRad, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Linear Regression model
LMFit1 <- train(SolarRad~., data = training, method = "lm", trControl=fitControl)
LMFit1
# Stop Cluster. After performing your tasks, stop your cluster.
stopCluster(cl)
#Import dataset
library("readr")
surveyData = read.csv('./input/CompleteResponses.csv')
surveyData
summary(surveyData)
#Explore data [to be commented out later]
str(surveyData)
table(surveyData)
view(surveyData)
View(surveyData)
is.na(surveyData)
is.na(surveyData$salary)
filter(surveyData, is.na(surveyData))
#Explore data [to be commented out later]
str(surveyData)
#Explore variation
ggplot(surveyData$elevel) + geom_bar(mapping = aes(x = cut))
#Transform variables
surveyData$elevel <- as.factor(surveyData$elevel)
#Explore variation
ggplot(surveyData$elevel) + geom_bar(mapping = aes(x = cut))
str(surveyData)
#Transform variables
surveyData$elevel <- as.factor(surveyData$elevel)
surveyData$car <- as.factor(surveyData$car)
surveyData$zipcode <- as.factor(surveyData$zipcode)
surveyData$brand <- as.factor(surveyData$brand)
######EDA##### [to be commented out later]
library(tidyverse)
str(surveyData)
tasummary(surveyData)
summary(surveyData)
is.na(surveyData)
#Explore variation
ggplot(surveyData$elevel) + geom_bar(mapping = aes(x = cut))
#Explore variation
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$elevel))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$car))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$zipcode))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$brand))
#Next, numerical
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$salary)
#Next, numerical
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$salary))
#Next, numerical
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$salary))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$age))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$credit))
#Explore covariance
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(cars, salary, FUN = median), y = salary))
cars
#Explore covariance
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(car, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(zipcode, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(credit, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(elevel, salary, FUN = median), y = salary))
#Explore correlation
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(car, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(zipcode, salary, FUN = median), y = salary))
library(caret)
##Explore collinearity and correlation
confusionMatrix(surveyData)
##Explore collinearity and correlation
library(corrplot)
install.packages("corrplot")
##Explore collinearity and correlation
library(corrplot)
corrplot(cor(surveyData))
cor(surveyData)
cor(select_if(surveyData, is.numeric())
c
cor(select_if(surveyData, is.numeric()))
cor(select_if(surveyData, is.numeric))
corrplot(cor(select_if(surveyData), is.numeric))
corrplot(cor(select_if(surveyData, is.numeric)))
corrplot(cor(filter(is.numeric, surveyData)))
corrplot(cor(filter(surveyData, is.numeric)))
filter(surveyData, is.numeric)
filter(surveyData, is.numeric(surveyData)))
filter(surveyData, is.numeric(surveyData))
corrplot(cor(base::filter(surveyData, is.numeric)))
corrplot(cor(base::Filter(surveyData, is.numeric)))
?filter
corrplot(cor(stats::Filter(surveyData, is.numeric)))
corrplot(cor(stats::filter(surveyData, is.numeric)))
?Filter
corrplot(cor(base::Filter(surveyData, is.numeric)))
corrplot(cor(base::Filter(is.numeric, surveyData)))
#Second for categorical and ordinals - vs salary
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(car, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = car, y = brand))
##Explore relations between independent and dependent variable
#Ordinal and categorical variables first
ggplot(data = diamonds) + geom_count(mapping = aes(x = car, y = brand))
##Explore relations between independent and dependent variable
#Ordinal and categorical variables first
ggplot(data = surveyData) + geom_count(mapping = aes(x = car, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = zipcode, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = elevel, y = brand))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, age, FUN = median), y = age))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, credit, FUN = median), y = credit))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, salary, FUN = median), y = salary))
##Explore relations between independent and dependent variable
#Ordinal and categorical variables first
ggplot(data = surveyData) + geom_count(mapping = aes(x = car, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = zipcode, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = elevel, y = brand))
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
training
str(brand)
str(training)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "randomForest", trControl=fitControl, tuneLength = 1)
library(e1071)
install.packages("e1071")
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
#Create training and test set with 75%
set.seed(998)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl, tuneLength = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl)
# Stop Cluster.
stopCluster(cl)
#Prepare clusters to optimze computing time
library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)
#Import dataset
library(readr)
surveyData = read.csv('./input/CompleteResponses.csv')
#Transform variables
surveyData$elevel <- as.factor(surveyData$elevel)
surveyData$car <- as.factor(surveyData$car)
surveyData$zipcode <- as.factor(surveyData$zipcode)
surveyData$brand <- as.factor(surveyData$brand)
######EDA##### [to be commented out later]
library(tidyverse)
library(caret)
str(surveyData)
summary(surveyData)
##Explore variation
#First categoricals and ordinals
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$elevel))
#Create training and test set with 75%
set.seed(998)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl)
rfFit1
#Save model to avoid future retraining
saveRDS(rfFit1, './output/RF.rds')
rfFit1
#Load model again
loadRDS(rfFit1, './output/RF.rds')
#Load model again
rfFit1 <- readRDS('./output/RF.rds')
#Prepare clusters to optimze computing time
library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)
#Import dataset
library(readr)
surveyData = read.csv('./input/CompleteResponses.csv')
#Transform variables
surveyData$elevel <- as.factor(surveyData$elevel)
surveyData$car <- as.factor(surveyData$car)
surveyData$zipcode <- as.factor(surveyData$zipcode)
surveyData$brand <- as.factor(surveyData$brand)
######EDA##### [to be commented out later]
library(tidyverse)
library(caret)
str(surveyData)
summary(surveyData)
##Explore variation
#First categoricals and ordinals
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$elevel))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$car))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$zipcode))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$brand))
#Next, numerical
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$salary))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$age))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$credit))
##Explore relations between independent and dependent variable
#Ordinal and categorical variables first
ggplot(data = surveyData) + geom_count(mapping = aes(x = car, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = zipcode, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = elevel, y = brand))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, age, FUN = median), y = age))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, credit, FUN = median), y = credit))
#Create training and test set with 75%
set.seed(998)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=fitControl)
#Predict
predictions <- predict(rfFit1, testing)
confusionMatrix(predictions, testing$brand)
#Try to solve class imbalance
summary(surveyData$brand)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("scale", "center"), trControl=fitControl)
#Predict
predictions <- predict(rfFit1, testing)
confusionMatrix(predictions, testing$brand)
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "down")
#Prepare clusters to optimze computing time
library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)
#Import dataset
library(readr)
surveyData = read.csv('./input/CompleteResponses.csv')
#Transform variables
surveyData$elevel <- as.factor(surveyData$elevel)
surveyData$car <- as.factor(surveyData$car)
surveyData$zipcode <- as.factor(surveyData$zipcode)
surveyData$brand <- as.factor(surveyData$brand)
######EDA##### [to be commented out later]
library(tidyverse)
library(caret)
str(surveyData)
summary(surveyData)
##Explore variation
#First categoricals and ordinals
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$elevel))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$car))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$zipcode))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$brand))
#Next, numerical
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$salary))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$age))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$credit))
##Explore relations between independent and dependent variable
#Ordinal and categorical variables first
ggplot(data = surveyData) + geom_count(mapping = aes(x = car, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = zipcode, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = elevel, y = brand))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, age, FUN = median), y = age))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, credit, FUN = median), y = credit))
#Create training and test set with 75%
set.seed(998)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "down")
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("scale", "center"), trControl=fitControl)
#Predict
predictions <- predict(rfFit1, testing)
confusionMatrix(predictions, testing$brand)
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "up")
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("scale", "center"), trControl=fitControl)
#Predict
predictions <- predict(rfFit1, testing)
confusionMatrix(predictions, testing$brand)
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "up")
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("center"), trControl=fitControl, metric = "Accuracy")
#Predict
predictions <- predict(rfFit1, testing)
confusionMatrix(predictions, testing$brand)
?postResample
postResample(predictions, surveyData)
confusionMatrix(predictions, surveyData)
confusionMatrix(predictions, surveyData$brand)
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "up", classProbs = TRUE)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("center"), trControl=fitControl, metric = "Accuracy")
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(training$brand~., data = training, method = "rf", preProcess = c("center"), trControl=fitControl, metric = "Accuracy")
VarImportance <- varImp(RF)
VarImportance <- varImp(rfFit1)
plot(VarImportance, main = "Top 15 most influencial Predictors", top = 15)
str(surveyData)
str(training)
#Create training and test set with 75%
set.seed(998)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "up", classProbs = TRUE)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(training$brand~., data = training, method = "rf", preProcess = c("center"), trControl=fitControl, metric = "Accuracy")
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("center"), trControl=fitControl, metric = "Accuracy")
training
str(training)
?make.names
?as.factor
surveyData$brand <- as.factor(surveyData$brand, levels("Acer", "Sony"),, labels = levels)
surveyData$brand <- as.factor(surveyData$brand, levels("Acer", "Sony"), labels = levels)
surveyData$brand <- as.factor(surveyData$brand, levels("Acer", "Sony"))
surveyData$brand <- as.factor(surveyData$brand, levels = c("Acer", "Sony"))
surveyData$brand <- as.factor(surveyData$brand, level = c("Acer", "Sony"))
surveyData$brand <- as.factor(surveyData$brand, labels = c("Acer", "Sony"))
surveyData$brand <- as.factor(surveyData$brand, levels = c(0,1),
labels = c("Acer", "Sony"))
surveyData$brand <- factor(surveyData$brand,
levels = c(0,1),
labels = c("Acer", "Sony"))
######EDA##### [to be commented out later]
library(tidyverse)
library(caret)
str(surveyData)
summary(surveyData)
##Explore variation
#First categoricals and ordinals
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$elevel))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$car))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$zipcode))
ggplot(surveyData) + geom_bar(mapping = aes(x = surveyData$brand))
#Next, numerical
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$salary))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$age))
ggplot(surveyData) + geom_histogram(mapping = aes(x = surveyData$credit))
##Explore relations between independent and dependent variable
#Ordinal and categorical variables first
ggplot(data = surveyData) + geom_count(mapping = aes(x = car, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = zipcode, y = brand))
ggplot(data = surveyData) + geom_count(mapping = aes(x = elevel, y = brand))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, age, FUN = median), y = age))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, credit, FUN = median), y = credit))
#Create training and test set with 75%
set.seed(998)
inTraining <- createDataPartition(surveyData$brand, p = .75, list = FALSE)
training <- surveyData[inTraining,]
testing <- surveyData[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, sampling = "up", classProbs = TRUE)
#train Random Forest Regression model with a tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", preProcess = c("center"), trControl=fitControl, metric = "Accuracy")
#Predict
predictions <- predict(rfFit1, testing)
confusionMatrix(predictions, surveyData$brand)
rfFit1
#Predict
predictions <- predict(rfFit1, testing)
length(testing$brand)
predictions
confusionMatrix(predictions, testing$brand)
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(salary, age, FUN = median), y = age))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, salary, FUN = median), y = salary))
ggplot(surveyData) + geom_boxplot(mapping = aes(x = reorder(brand, credit, FUN = median), y = credit))
WholeYear = read.csv('./input/WholeYear.csv')
# Required
library(doParallel)
# Create Cluster with desired number of cores. Don't use them all! Your computer is running other processes.
cl <- makeCluster(3)
# Register Cluster
registerDoParallel(cl)
y
#load library and set seed
library(caret)
set.seed(998)
#create a 20% sample of the data
WholeYear <- WholeYear[sample(1:nrow(WholeYear), 7000,replace=FALSE),]
# define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(WholeYear$SolarRad, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#dataframe for manual tuning of mtry
rfGrid <- expand.grid(mtry=c(1,2,3))
WholeYear = read.csv('./input/WholeYear.csv')
# Required
library(doParallel)
# Create Cluster with desired number of cores. Don't use them all! Your computer is running other processes.
cl <- makeCluster(3)
# Register Cluster
registerDoParallel(cl)
#load library and set seed
library(caret)
set.seed(998)
#create a 20% sample of the data
WholeYear <- WholeYear[sample(1:nrow(WholeYear), 7000,replace=FALSE),]
# define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(WholeYear$SolarRad, p = .75, list = FALSE)
training <- WholeYear[inTraining,]
testing <- WholeYear[-inTraining,]
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
#dataframe for manual tuning of mtry
rfGrid <- expand.grid(mtry=c(1,2,3))
#train Random Forest Regression model
#note the system time wrapper. system.time()
#this is used to measure process execution time
system.time(rfFitm1 <- train(SolarRad~., data = training, method = "rf", trControl=fitControl, tuneGrid=rfGrid))
