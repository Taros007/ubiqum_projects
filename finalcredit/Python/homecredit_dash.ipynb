{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "# Utilities\n",
    "import sys\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    \"\"\"Convert pandas data types for memory reduction.\"\"\"\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif set(df[c].unique()) == {0, 1}:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory before converting types: 4.38 gb.\n",
      "Total memory after converting types: 2.03 gb.\n"
     ]
    }
   ],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "\n",
    "# Join together training and testing\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "number_clients = app.shape[0]\n",
    "\n",
    "# Need `SK_ID_CURR` in every dataset\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], \n",
    "                                      on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "print(f\"\"\"Total memory before converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Convert types to reduce memory usage\n",
    "app = convert_types(app)\n",
    "bureau = convert_types(bureau)\n",
    "bureau_balance = convert_types(bureau_balance)\n",
    "cash = convert_types(cash)\n",
    "credit = convert_types(credit)\n",
    "previous = convert_types(previous)\n",
    "installments = convert_types(installments)\n",
    "\n",
    "print(f\"\"\"Total memory after converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object memory usage.\n",
      "0.027462848 gb\n",
      "Category memory usage.\n",
      "0.015448612 gb\n",
      "Length of data:  1716428\n",
      "Number of unique categories:  15\n"
     ]
    }
   ],
   "source": [
    "print('Object memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('object').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Category memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('category').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Length of data: ', bureau.shape[0])\n",
    "print('Number of unique categories: ', bureau['CREDIT_TYPE'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition):\n",
    "    \"\"\"Creates and saves a dataset with only the users in `user_list`.\"\"\"\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = '../input/partitions/p%d' % (partition + 1)\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        # Subset based on user list\n",
    "        app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "        bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "\n",
    "        # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "        bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "        installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # Save data to the directory\n",
    "        app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "        bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "        bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "        cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "        credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "        previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "        installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "        if partition % 10 == 0:\n",
    "            print('Saved all files in partition {} to {}.'.format(partition + 1, directory))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into 104 chunks\n",
    "chunk_size = app.shape[0] // 103\n",
    "\n",
    "# Construct an id list\n",
    "id_list = [list(app.iloc[i:i+chunk_size].index) for i in range(0, app.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids in id_list:         356255.\n",
      "Total length of application data: 356255.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Sanity check that we have not missed any ids\n",
    "print('Number of ids in id_list:         {}.'.format(len(list(chain(*id_list)))))\n",
    "print('Total length of application data: {}.'.format(len(app)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all files in partition 1 to ../input/partitions/p1.\n",
      "Saved all files in partition 11 to ../input/partitions/p11.\n",
      "Saved all files in partition 21 to ../input/partitions/p21.\n",
      "Saved all files in partition 31 to ../input/partitions/p31.\n",
      "Saved all files in partition 41 to ../input/partitions/p41.\n",
      "Saved all files in partition 51 to ../input/partitions/p51.\n",
      "Saved all files in partition 61 to ../input/partitions/p61.\n",
      "Saved all files in partition 71 to ../input/partitions/p71.\n",
      "Saved all files in partition 81 to ../input/partitions/p81.\n",
      "Saved all files in partition 91 to ../input/partitions/p91.\n",
      "Saved all files in partition 101 to ../input/partitions/p101.\n",
      "Partitioning took 1358 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "for i, ids in enumerate(id_list):\n",
    "    # Create a partition based on the ids\n",
    "    create_partition(ids, i)\n",
    "    \n",
    "end = timer()\n",
    "print(f'Partitioning took {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "feature_defs = ft.load_features('../input/features.txt')\n",
    "print(len(feature_defs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path.\n",
    "       Returns a dictionary with the entityset and the number used for saving the feature matrix.\"\"\"\n",
    "    \n",
    "    partition_num = int(path[21:])\n",
    "    \n",
    "    # Read in data\n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return ({'es': es, 'num': partition_num})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 3458, Columns: 122]\n",
       "    bureau [Rows: 16097, Columns: 17]\n",
       "    previous [Rows: 16204, Columns: 37]\n",
       "    bureau_balance [Rows: 166374, Columns: 4]\n",
       "    cash [Rows: 96632, Columns: 8]\n",
       "    installments [Rows: 129130, Columns: 8]\n",
       "    credit [Rows: 35694, Columns: 23]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just a test of the function above\n",
    "es1_dict = entityset_from_partition('../input/partitions/p1')\n",
    "es1_dict['es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es_dict, feature_defs, return_fm = False):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature definitions. \n",
    "    Saves feature matrix based on partition.\"\"\" \n",
    "    \n",
    "    # Extract the entityset\n",
    "    es = es_dict['es']\n",
    "    \n",
    "    # Calculate the feature matrix and save\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_defs, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = es['app'].df.shape[0])\n",
    "    \n",
    "    feature_matrix.to_csv('../input/fm/p%d_fm.csv' % es_dict['num'], index = True)\n",
    "    \n",
    "    if return_fm:\n",
    "        return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 1820)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)\n",
    "\n",
    "start = timer()\n",
    "fm1 = feature_matrix_from_entityset(es1_dict, feature_defs, return_fm = True)\n",
    "end = timer()\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing one feature matrix took 65.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Computing one feature matrix took {round(end - start, 2)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Free up all system memory\n",
    "gc.enable()\n",
    "del app, bureau, bureau_balance, previous, credit, cash, installments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:32865': 1,\n",
       " 'tcp://127.0.0.1:36381': 1,\n",
       " 'tcp://127.0.0.1:43447': 1,\n",
       " 'tcp://127.0.0.1:45947': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.bag as db\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Use all 8 cores\n",
    "client = Client(processes = True)\n",
    "\n",
    "client.ncores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p1',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p4',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p6',\n",
       " '../input/partitions/p7',\n",
       " '../input/partitions/p8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/p%d' %  i for i in range(1, 105)]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<feature..., npartitions=104>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bag object\n",
    "b = db.from_sequence(paths)\n",
    "\n",
    "# Map entityset function\n",
    "b = b.map(entityset_from_partition)\n",
    "\n",
    "# Map feature matrix function\n",
    "b = b.map(feature_matrix_from_entityset, feature_defs = feature_defs)\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = timer()\n",
    "b.compute()\n",
    "overall_end = timer()\n",
    "\n",
    "print(f\"Total Time Elapsed: {round(overall_end - overall_start, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in 104 feature matrices took 143 seconds.\n",
      "Final Feature Matrix Shape: (356255, 1821)\n",
      "Concatenation time: 6.18 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Base directory for feature matrices\n",
    "base = '../input/fm/'\n",
    "fm_paths = [base + p for p in os.listdir(base) if 'fm.csv' in p]\n",
    "\n",
    "#First we read in the dataframes and place them in a list\n",
    "\n",
    "read_start = timer()\n",
    "fms = [pd.read_csv(path) for path in fm_paths]\n",
    "read_end = timer()\n",
    "\n",
    "print(f'Reading in {len(fms)} feature matrices took {round(read_end - read_start)} seconds.')\n",
    "\n",
    "#Then we concatenate all the dataframes in the list along the first axis - meaning that we add the rows to each other.\n",
    "concat_start = timer()\n",
    "feature_matrix = pd.concat(fms, axis = 0)\n",
    "concat_end = timer()\n",
    "\n",
    "print('Final Feature Matrix Shape:', feature_matrix.shape)\n",
    "print(f\"Concatenation time: {round(concat_end - concat_start, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENTILE(MIN(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>PERCENTILE(MIN(installments.AMT_INSTALMENT))</th>\n",
       "      <th>PERCENTILE(MIN(installments.AMT_PAYMENT))</th>\n",
       "      <th>PERCENTILE(MEAN(installments.NUM_INSTALMENT_VERSION))</th>\n",
       "      <th>PERCENTILE(MEAN(installments.NUM_INSTALMENT_NUMBER))</th>\n",
       "      <th>PERCENTILE(MEAN(installments.DAYS_INSTALMENT))</th>\n",
       "      <th>PERCENTILE(MEAN(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>PERCENTILE(MEAN(installments.AMT_INSTALMENT))</th>\n",
       "      <th>PERCENTILE(MEAN(installments.AMT_PAYMENT))</th>\n",
       "      <th>PERCENTILE(COUNT(installments))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>224374</td>\n",
       "      <td>27643.5</td>\n",
       "      <td>832500.0</td>\n",
       "      <td>832500.0</td>\n",
       "      <td>195750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183164</td>\n",
       "      <td>0.123343</td>\n",
       "      <td>0.287388</td>\n",
       "      <td>0.095899</td>\n",
       "      <td>0.872649</td>\n",
       "      <td>0.332717</td>\n",
       "      <td>0.328708</td>\n",
       "      <td>0.182855</td>\n",
       "      <td>0.199198</td>\n",
       "      <td>0.826634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>224375</td>\n",
       "      <td>46899.0</td>\n",
       "      <td>1227901.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021122</td>\n",
       "      <td>0.257478</td>\n",
       "      <td>0.273204</td>\n",
       "      <td>0.656491</td>\n",
       "      <td>0.711687</td>\n",
       "      <td>0.064755</td>\n",
       "      <td>0.065063</td>\n",
       "      <td>0.409497</td>\n",
       "      <td>0.384521</td>\n",
       "      <td>0.647340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>224376</td>\n",
       "      <td>16875.0</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530990</td>\n",
       "      <td>0.665742</td>\n",
       "      <td>0.441875</td>\n",
       "      <td>0.525439</td>\n",
       "      <td>0.790163</td>\n",
       "      <td>0.372803</td>\n",
       "      <td>0.372495</td>\n",
       "      <td>0.284613</td>\n",
       "      <td>0.243293</td>\n",
       "      <td>0.712406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>224377</td>\n",
       "      <td>29110.5</td>\n",
       "      <td>898326.0</td>\n",
       "      <td>643500.0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555196</td>\n",
       "      <td>0.415665</td>\n",
       "      <td>0.395621</td>\n",
       "      <td>0.848751</td>\n",
       "      <td>0.593124</td>\n",
       "      <td>0.486278</td>\n",
       "      <td>0.486586</td>\n",
       "      <td>0.828862</td>\n",
       "      <td>0.850139</td>\n",
       "      <td>0.603383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>224378</td>\n",
       "      <td>21888.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576781</td>\n",
       "      <td>0.678693</td>\n",
       "      <td>0.058434</td>\n",
       "      <td>0.340888</td>\n",
       "      <td>0.819303</td>\n",
       "      <td>0.622880</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.566142</td>\n",
       "      <td>0.305273</td>\n",
       "      <td>0.826634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  SK_ID_CURR  AMT_ANNUITY  AMT_CREDIT  AMT_GOODS_PRICE  \\\n",
       "0      0      224374      27643.5    832500.0         832500.0   \n",
       "1      1      224375      46899.0   1227901.5        1129500.0   \n",
       "2      2      224376      16875.0    337500.0         337500.0   \n",
       "3      3      224377      29110.5    898326.0         643500.0   \n",
       "4      4      224378      21888.0    450000.0         450000.0   \n",
       "\n",
       "   AMT_INCOME_TOTAL  AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0          195750.0                        0.0                         0.0   \n",
       "1          202500.0                        0.0                         0.0   \n",
       "2          112500.0                        0.0                         0.0   \n",
       "3          112500.0                        0.0                         0.0   \n",
       "4          450000.0                        0.0                         0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  ...  \\\n",
       "0                        1.0                        2.0  ...   \n",
       "1                        0.0                        0.0  ...   \n",
       "2                        0.0                        1.0  ...   \n",
       "3                        2.0                        3.0  ...   \n",
       "4                        0.0                        0.0  ...   \n",
       "\n",
       "   PERCENTILE(MIN(installments.DAYS_ENTRY_PAYMENT))  \\\n",
       "0                                          0.183164   \n",
       "1                                          0.021122   \n",
       "2                                          0.530990   \n",
       "3                                          0.555196   \n",
       "4                                          0.576781   \n",
       "\n",
       "   PERCENTILE(MIN(installments.AMT_INSTALMENT))  \\\n",
       "0                                      0.123343   \n",
       "1                                      0.257478   \n",
       "2                                      0.665742   \n",
       "3                                      0.415665   \n",
       "4                                      0.678693   \n",
       "\n",
       "   PERCENTILE(MIN(installments.AMT_PAYMENT))  \\\n",
       "0                                   0.287388   \n",
       "1                                   0.273204   \n",
       "2                                   0.441875   \n",
       "3                                   0.395621   \n",
       "4                                   0.058434   \n",
       "\n",
       "   PERCENTILE(MEAN(installments.NUM_INSTALMENT_VERSION))  \\\n",
       "0                                           0.095899       \n",
       "1                                           0.656491       \n",
       "2                                           0.525439       \n",
       "3                                           0.848751       \n",
       "4                                           0.340888       \n",
       "\n",
       "   PERCENTILE(MEAN(installments.NUM_INSTALMENT_NUMBER))  \\\n",
       "0                                           0.872649      \n",
       "1                                           0.711687      \n",
       "2                                           0.790163      \n",
       "3                                           0.593124      \n",
       "4                                           0.819303      \n",
       "\n",
       "   PERCENTILE(MEAN(installments.DAYS_INSTALMENT))  \\\n",
       "0                                        0.332717   \n",
       "1                                        0.064755   \n",
       "2                                        0.372803   \n",
       "3                                        0.486278   \n",
       "4                                        0.622880   \n",
       "\n",
       "   PERCENTILE(MEAN(installments.DAYS_ENTRY_PAYMENT))  \\\n",
       "0                                           0.328708   \n",
       "1                                           0.065063   \n",
       "2                                           0.372495   \n",
       "3                                           0.486586   \n",
       "4                                           0.624113   \n",
       "\n",
       "   PERCENTILE(MEAN(installments.AMT_INSTALMENT))  \\\n",
       "0                                       0.182855   \n",
       "1                                       0.409497   \n",
       "2                                       0.284613   \n",
       "3                                       0.828862   \n",
       "4                                       0.566142   \n",
       "\n",
       "   PERCENTILE(MEAN(installments.AMT_PAYMENT))  PERCENTILE(COUNT(installments))  \n",
       "0                                    0.199198                         0.826634  \n",
       "1                                    0.384521                         0.647340  \n",
       "2                                    0.243293                         0.712406  \n",
       "3                                    0.850139                         0.603383  \n",
       "4                                    0.305273                         0.826634  \n",
       "\n",
       "[5 rows x 1822 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.reset_index(inplace = True)\n",
    "feature_matrix.to_csv('../input/feature_matrix.csv', index = False)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empty memory\n",
    "import gc\n",
    "gc.enable()\n",
    "del feature_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.close()\n",
    "b.terminate()\n",
    "b.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling 10% of the original data\n",
    "train = feature_matrix[feature_matrix['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active) not in data\n",
      "SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n"
     ]
    }
   ],
   "source": [
    "for col in ['SUM(bureau.PREVIOUS_OTHER_LOAN_RATE)', 'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed)',\n",
    "            'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active)', 'SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE)']:\n",
    "    try:\n",
    "        train[col] = train[col].astype(np.float32)\n",
    "    except:\n",
    "        print(f'{col} not in data')\n",
    "    \n",
    "for col in train:\n",
    "    if train[col].dtype == 'bool':\n",
    "        train[col] = train[col].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 2094)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.get_dummies(train)\n",
    "n_features_start = train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1806)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns with duplicate items\n",
    "x, idx, inv, counts = np.unique(train, axis = 1, return_index = True, return_inverse=True, return_counts=True)\n",
    "train = train.iloc[:, idx]\n",
    "n_non_unique_columns = n_features_start - train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1788)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove columns with >=90% missing values\n",
    "missing_threshold = 90\n",
    "\n",
    "# Find missing and percentage\n",
    "missing = pd.DataFrame(train.isnull().sum())\n",
    "missing['percent'] = 100 * (missing[0] / train.shape[0])\n",
    "missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "# Missing above threshold\n",
    "missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "n_missing_cols = len(missing_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in missing_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1769)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove zero variance columns\n",
    "unique_counts = pd.DataFrame(train.nunique()).sort_values(0, ascending = True)\n",
    "zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in zero_variance_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "PERCENTILE(TARGET)\n"
     ]
    }
   ],
   "source": [
    "#Remove target columns (accidentally left in earlier code)\n",
    "for col in train:\n",
    "    if 'TARGET' in col:\n",
    "        print(col)\n",
    "        \n",
    "train.drop(columns = 'PERCENTILE(TARGET)', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns with high collinearity with other column\n",
    "correlation_threshold = 0.95\n",
    "\n",
    "corr_matrix = train.corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Select the features with correlations above the threshold\n",
    "# Need to use the absolute value\n",
    "to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1057)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[[x for x in train if x not in to_drop]]\n",
    "n_collinear = len(to_drop)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns removed:  1033\n"
     ]
    }
   ],
   "source": [
    "total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear + 1\n",
    "print('Total columns removed: ', total_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../input/feature_matrix_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature_matrix, missing_threshold=90, correlation_threshold=0.95):\n",
    "    \"\"\"Feature selection for a dataframe.\"\"\"\n",
    "    \n",
    "    feature_matrix = pd.get_dummies(feature_matrix)\n",
    "    n_features_start = feature_matrix.shape[1]\n",
    "    print('Original shape: ', feature_matrix.shape)\n",
    "\n",
    "    _, idx = np.unique(feature_matrix, axis = 1, return_index = True)\n",
    "    feature_matrix = feature_matrix.iloc[:, idx]\n",
    "    n_non_unique_columns = n_features_start - feature_matrix.shape[1]\n",
    "    print('{}  non-unique valued columns.'.format(n_non_unique_columns))\n",
    "\n",
    "    # Find missing and percentage\n",
    "    missing = pd.DataFrame(feature_matrix.isnull().sum())\n",
    "    missing['percent'] = 100 * (missing[0] / feature_matrix.shape[0])\n",
    "    missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "    # Missing above threshold\n",
    "    missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "    n_missing_cols = len(missing_cols)\n",
    "\n",
    "    # Remove missing columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n",
    "    print('{} missing columns with threshold: {}.'.format(n_missing_cols,\n",
    "                                                                        missing_threshold))\n",
    "    \n",
    "    # Zero variance\n",
    "    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n",
    "    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "    n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "    # Remove zero variance columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n",
    "    print('{} zero variance columns.'.format(n_zero_variance_cols))\n",
    "    \n",
    "    # Correlations\n",
    "    corr_matrix = feature_matrix.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    n_collinear = len(to_drop)\n",
    "    \n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n",
    "    print('{} collinear columns removed with threshold: {}.'.format(n_collinear,\n",
    "                                                                          correlation_threshold))\n",
    "    \n",
    "    total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear\n",
    "    \n",
    "    print('Total columns removed: ', total_removed)\n",
    "    print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 244)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = pd.read_csv('../input/application_train.csv')\n",
    "fm = fm.sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:  (30751, 244)\n",
      "0  non-unique valued columns.\n",
      "0 missing columns with threshold: 90.\n",
      "2 zero variance columns.\n",
      "37 collinear columns removed with threshold: 0.95.\n",
      "Total columns removed:  39\n",
      "Shape after feature selection: (30751, 205).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>ORGANIZATION_TYPE_Trade: type 5</th>\n",
       "      <th>NAME_INCOME_TYPE_Maternity leave</th>\n",
       "      <th>FLAG_DOCUMENT_7</th>\n",
       "      <th>...</th>\n",
       "      <th>BASEMENTAREA_MEDI</th>\n",
       "      <th>ELEVATORS_MODE</th>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MODE</th>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77158</th>\n",
       "      <td>-14017</td>\n",
       "      <td>-3747</td>\n",
       "      <td>-2384.0</td>\n",
       "      <td>-1046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306191</th>\n",
       "      <td>-16520</td>\n",
       "      <td>-4275</td>\n",
       "      <td>-3198.0</td>\n",
       "      <td>-82</td>\n",
       "      <td>-1542.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64916</th>\n",
       "      <td>-20741</td>\n",
       "      <td>365243</td>\n",
       "      <td>-1882.0</td>\n",
       "      <td>-4296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81133</th>\n",
       "      <td>-9685</td>\n",
       "      <td>-318</td>\n",
       "      <td>-378.0</td>\n",
       "      <td>-1763</td>\n",
       "      <td>-1090.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231607</th>\n",
       "      <td>-20891</td>\n",
       "      <td>-413</td>\n",
       "      <td>-3154.0</td>\n",
       "      <td>-3595</td>\n",
       "      <td>-1696.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  DAYS_ID_PUBLISH  \\\n",
       "77158       -14017          -3747            -2384.0            -1046   \n",
       "306191      -16520          -4275            -3198.0              -82   \n",
       "64916       -20741         365243            -1882.0            -4296   \n",
       "81133        -9685           -318             -378.0            -1763   \n",
       "231607      -20891           -413            -3154.0            -3595   \n",
       "\n",
       "        DAYS_LAST_PHONE_CHANGE  FLAG_DOCUMENT_12  FLAG_DOCUMENT_2  \\\n",
       "77158                      0.0                 0                0   \n",
       "306191                 -1542.0                 0                0   \n",
       "64916                      0.0                 0                0   \n",
       "81133                  -1090.0                 0                0   \n",
       "231607                 -1696.0                 0                0   \n",
       "\n",
       "        ORGANIZATION_TYPE_Trade: type 5  NAME_INCOME_TYPE_Maternity leave  \\\n",
       "77158                                 0                                 0   \n",
       "306191                                0                                 0   \n",
       "64916                                 0                                 0   \n",
       "81133                                 0                                 0   \n",
       "231607                                0                                 0   \n",
       "\n",
       "        FLAG_DOCUMENT_7  ...  BASEMENTAREA_MEDI  ELEVATORS_MODE  \\\n",
       "77158                 0  ...                NaN             NaN   \n",
       "306191                0  ...                NaN             NaN   \n",
       "64916                 0  ...                NaN             NaN   \n",
       "81133                 0  ...                NaN             NaN   \n",
       "231607                0  ...                NaN             NaN   \n",
       "\n",
       "        COMMONAREA_MODE  NONLIVINGAPARTMENTS_MODE  NONLIVINGAPARTMENTS_MEDI  \\\n",
       "77158               NaN                       NaN                       NaN   \n",
       "306191              NaN                       NaN                       NaN   \n",
       "64916               NaN                       NaN                       NaN   \n",
       "81133               NaN                       NaN                       NaN   \n",
       "231607              NaN                       NaN                       NaN   \n",
       "\n",
       "        NONLIVINGAREA_MODE  LANDAREA_MODE  LIVINGAPARTMENTS_MODE  \\\n",
       "77158                  NaN            NaN                    NaN   \n",
       "306191                 NaN            NaN                    NaN   \n",
       "64916                  NaN            NaN                    NaN   \n",
       "81133                  NaN            NaN                    NaN   \n",
       "231607                 NaN            NaN                    NaN   \n",
       "\n",
       "        FLOORSMIN_AVG  YEARS_BUILD_AVG  \n",
       "77158             NaN              NaN  \n",
       "306191            NaN              NaN  \n",
       "64916             NaN              NaN  \n",
       "81133             NaN              NaN  \n",
       "231607            NaN              NaN  \n",
       "\n",
       "[5 rows x 205 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = feature_selection(fm, 90, 0.95)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_csv('../input/features_default_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356255, 1821)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1d6c1671c7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bool'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Add the one-hot encoded columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fm' is not defined"
     ]
    }
   ],
   "source": [
    "# Read in sample and full data\n",
    "#sample = pd.read_csv('../input/feature_matrix_sample.csv')\n",
    "#fm = pd.read_csv('../input/feature_matrix.csv')\n",
    "\n",
    "print(feature_matrix.shape)\n",
    "\n",
    "# One hot encoding\n",
    "cat = pd.get_dummies(feature_matrix.select_dtypes('object'))\n",
    "\n",
    "# Convert the column types\n",
    "for col in feature_matrix:\n",
    "    if feature_matrix[col].dtype == 'bool':\n",
    "        feature_matrix[col] = fm[col].astype(np.uint8)\n",
    "        \n",
    "# Add the one-hot encoded columns\n",
    "feature_matrix = feature_matrix.select_dtypes(['number'])\n",
    "feature_matrix = feature_matrix.concat([fm, cat], axis = 1)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empty memory\n",
    "import gc\n",
    "gc.enable()\n",
    "del fm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
